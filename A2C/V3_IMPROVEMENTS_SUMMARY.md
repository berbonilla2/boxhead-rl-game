# A2C V3 - Improvements Based on V2 Analysis

## üìä V2 Performance Analysis

### Training Results (V2)
- **Total Episodes**: 248
- **Mean Reward**: 67.57 ¬± 95.10
- **Mean Episode Length**: 603.85 steps
- **Completion Rate**: 10.08% (only 25/248 episodes reached 1000 steps)
- **Max Reward**: 386.66 (showing potential)
- **Min Reward**: -117.83 (high variance)

### Problems Identified in V2
1. **High Variance** (std: 95.10) - Inconsistent performance
2. **Low Completion Rate** (10%) - Poor survival
3. **Evaluation Failure** (mean eval: -120.75) - Worse than training
4. **Overfitting** - Evaluation much worse than training
5. **State Complexity** - 60 features may be too much
6. **Network Overfitting** - 512 hidden units, 5 layers
7. **Unicode Error** - Log writing failed with special characters

---

## ‚ú® V3 Improvements

### 1. **Reduced State Complexity**
**60 ‚Üí 42 features (-30%)**

| Component | V2 | V3 | Change |
|-----------|----|----|--------|
| Position & Status | 8 | 8 | Same |
| Enemy Tracking | 30 (5 enemies) | 18 (3 enemies) | -40% |
| Resources & Items | 8 | 8 | Same |
| Map Layout | 8 | 6 | Simplified |
| Temporal | 4 | 2 | -50% |
| **Total** | **60** | **42** | **-30%** |

**Rationale**: 
- Tracking 5 enemies was information overload
- 3 enemies is enough for tactical awareness
- Reduced temporal history (4‚Üí2) focuses on immediate patterns

---

### 2. **Simplified Network Architecture**

**V2**: 60 ‚Üí 256 ‚Üí 512 ‚Üí 512 ‚Üí 512 ‚Üí 512 (~2M params)  
**V3**: 42 ‚Üí 256 ‚Üí 384 ‚Üí 384 ‚Üí 384 (~1.2M params, -40%)

| Layer | V2 Size | V3 Size | Change |
|-------|---------|---------|--------|
| Feature Extractor | 512 | 384 | -25% |
| Policy Head | [512, 256, 128] | [384, 192] | Simplified |
| Value Head | [512, 256, 128] | [384, 192] | Simplified |
| Dropout | 0.1, 0.1, 0.05 | 0.15, 0.12, 0.08 | +50% |

**Rationale**:
- Smaller network = less overfitting
- V2's 5-layer network was too complex
- Increased dropout for regularization

---

### 3. **Optimized Hyperparameters**

| Parameter | V2 | V3 | Rationale |
|-----------|----|----|-----------|
| Learning Rate | 2e-4 | **3e-4** | Faster learning, less stagnation |
| Warmup | 15% | **20%** | More stable initialization |
| N-steps | 24 | **16** | Better for shorter episodes |
| Gamma | 0.997 | **0.99** | Faster credit assignment |
| Entropy | 0.025 | **0.04** | +60% exploration |
| Episodes | 150 | **200** | More training data |

**Key Changes**:
- **Higher LR**: V2 was learning too slowly
- **Reduced n-steps**: 24 was too long for unstable episodes
- **Lower gamma**: Faster learning from immediate rewards
- **Higher entropy**: V2 wasn't exploring enough

---

### 4. **Improved Environment**

#### Simplified Weapon System
**V2**: 3 weapons (Pistol, Shotgun, Rifle)  
**V3**: 2 weapons (Pistol, Shotgun)

#### Balanced Difficulty
**V2**: Started with 5 zombies + 2 demons  
**V3**: Started with 3 zombies + 1 demon

#### Better Reward Shaping
| Reward Component | V2 | V3 | Change |
|------------------|----|----|--------|
| Survival | +0.1 | +0.12 | +20% |
| Health | +0.2 √ó health | +0.18 √ó health | -10% (smoother) |
| Optimal distance | +0.4 | +0.35 | -12% (more forgiving) |
| Kill (demon) | +8.0 | +6.0 | -25% (balanced) |
| Kill (zombie) | +5.0 | +4.0 | -20% |
| Hit reward | +1.0 | +0.8 | -20% |
| Death penalty | -50 | -30 | -40% (less harsh) |
| Miss penalty | -0.05 | -0.02 | -60% (less punishing) |
| Collision penalty | -0.8 | -0.5 | -37% |

**Rationale**:
- V2 had too many harsh penalties causing negative spirals
- Reduced kill rewards to focus on survival
- Smoother reward gradients for better learning

---

### 5. **Enhanced Regularization**

**V2**:
- Dropout: 0.1, 0.1, 0.05
- No L2 regularization
- Gradient clipping: 0.5

**V3**:
- Dropout: 0.15, 0.12, 0.08 (+50%)
- Same gradient clipping
- Smaller network (implicit regularization)

---

### 6. **Fixed Technical Issues**

**Unicode Encoding Error** (V2):
```python
UnicodeEncodeError: 'charmap' codec can't encode character '\u2192'
```

**Fixed in V3**:
```python
with open(log_path, "a", encoding='utf-8') as f:
    f.write(log_entry)
```

All file operations now use UTF-8 encoding.

---

## üìà Expected Improvements

### Performance Predictions

| Metric | V2 Actual | V3 Target | Expected Improvement |
|--------|-----------|-----------|---------------------|
| Mean Reward | 67.57 | 100-150 | +48% to +122% |
| Mean Length | 603.85 | 700-850 | +16% to +41% |
| Completion Rate | 10.08% | 20-30% | 2-3x better |
| Variance (std) | 95.10 | 60-75 | -21% to -37% |
| Eval Performance | -120.75 | 50-100+ | Positive instead of negative |

### Why V3 Should Perform Better

1. **Reduced Overfitting**: Smaller network generalizes better
2. **Better Exploration**: Higher entropy finds better strategies
3. **Faster Learning**: Higher LR, lower gamma
4. **More Stable**: Longer warmup, better reward shaping
5. **Simpler State**: Less noise, clearer signals
6. **Less Penalty Spirals**: Reduced harsh penalties

---

## üèóÔ∏è Architecture Comparison

### V2 Architecture
```
Input: 60 features
    ‚Üì
256 ‚Üí 512 ‚Üí 512 ‚Üí 512 ‚Üí 512 (5 layers)
    ‚Üì
Policy: 512 ‚Üí 256 ‚Üí 128 ‚Üí 6
Value:  512 ‚Üí 256 ‚Üí 128 ‚Üí 1

Parameters: ~2M
Dropout: 0.1, 0.1, 0.05
```

### V3 Architecture
```
Input: 42 features
    ‚Üì
256 ‚Üí 384 ‚Üí 384 ‚Üí 384 (4 layers)
    ‚Üì
Policy: 384 ‚Üí 192 ‚Üí 6
Value:  384 ‚Üí 192 ‚Üí 1

Parameters: ~1.2M (-40%)
Dropout: 0.15, 0.12, 0.08 (+50%)
```

---

## üéØ State Representation Changes

### V2 State (60 features)
```
Position & Status: 8
Enemy Info (5 enemies √ó 6): 30
Resources: 5
Items: 6
Map Layout: 8
Temporal (4 actions): 4
```

### V3 State (42 features)
```
Position & Status: 8
Enemy Info (3 enemies √ó 6): 18  ‚Üê Reduced
Resources & Items: 8
Map Tactical: 6  ‚Üê Simplified
Temporal (2 actions): 2  ‚Üê Reduced
```

**Key Simplifications**:
- 5 ‚Üí 3 enemies (focus on nearest threats)
- 4 ‚Üí 2 action history (recent patterns only)
- Removed redundant map features
- Combined resource/item features

---

## üîß Training Differences

| Aspect | V2 | V3 |
|--------|----|----|
| State Features | 60 | 42 |
| Network Params | ~2M | ~1.2M |
| Total Timesteps | 150k | 200k |
| Learning Rate | 2e-4 ‚Üí decay | 3e-4 ‚Üí decay |
| Exploration | 0.025 entropy | 0.04 entropy |
| Warmup | 15% | 20% |
| Regularization | Moderate | High |
| Reward Structure | Complex | Simplified |
| Environment | Hard (5 enemies) | Balanced (3 enemies) |

---

## üìù Training Recommendations

### Before Training
1. Review state features in V3_STATE_SPECIFICATION.md
2. Ensure ~60 minutes available for training
3. Monitor the real-time plots
4. Watch for improvements in:
   - Increasing mean rewards
   - Decreasing variance
   - Higher completion rates
   - Positive evaluation scores

### During Training
1. **Good Signs**:
   - Rewards trending upward
   - More episodes reaching 1000 steps
   - Evaluation > training (good generalization)
   - Entropy decreasing gradually
   - Losses stabilizing

2. **Bad Signs**:
   - Rewards oscillating wildly
   - Evaluation << training (overfitting)
   - Entropy dropping to near zero
   - Losses increasing

### After Training
1. Compare V3 vs V2 results
2. If V3 > V2: Continue iterating
3. If V3 ‚â§ V2: Consider different approach
4. Analyze which improvements helped most

---

## üéÅ Files Created

1. **`enhanced_boxhead_env_v3.py`** (500+ lines)
   - 42-feature state representation
   - Simplified environment
   - Better reward shaping
   - Balanced difficulty

2. **`A2C_V3_train.py`** (650+ lines)
   - Optimized architecture
   - Better hyperparameters
   - Enhanced callbacks
   - UTF-8 encoding fix

3. **`v2_results_summary.txt`**
   - Complete V2 analysis
   - Problems identified
   - Recommendations

4. **`V3_IMPROVEMENTS_SUMMARY.md`** (This file)
   - All improvements documented
   - Comparison tables
   - Training guide

---

## ‚ö†Ô∏è Notes

**Status**: ‚úÖ Ready for training (scripts prepared)

**Expected Training Time**: 50-70 minutes

**Expected Outcome**: 
- If successful: 50-120% improvement over V2
- Positive evaluation scores (vs -120 in V2)
- 20-30% completion rate (vs 10% in V2)
- Much lower variance (better consistency)

**Next Steps After V3**:
- If V3 improves: Create V4 with fine-tuning
- If V3 plateaus: Try PPO algorithm
- Consider curriculum learning
- Experiment with reward scaling

---

**Created**: October 18, 2025  
**Status**: Ready for Training  
**Model Version**: v3  
**Based on**: V2 analysis and results  

---

## üöÄ Quick Start

```bash
# Activate environment
gameEnv\Scripts\activate

# Navigate to folder
cd A2C

# Train V3 (when ready)
python A2C_V3_train.py
```

**Remember**: V3 is designed to fix V2's problems. Monitor training carefully and compare results!

